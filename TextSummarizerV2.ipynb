{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.Installations\n",
        "\n",
        "Install necessary libraries, including `optuna` for hyperparameter optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install pandas numpy torch scikit-learn transformers sentencepiece evaluate rouge_score accelerate optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup \n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk \n",
        "import optuna \n",
        "\n",
        "from transformers import (\n",
        "    MT5Tokenizer,\n",
        "    MT5ForConditionalGeneration\n",
        ")\n",
        "from transformers.modeling_utils import PreTrainedModel \n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase \n",
        "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union \n",
        "\n",
        "import evaluate \n",
        "import traceback\n",
        "import gc \n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt', quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME_OR_PATH = \"google/mt5-small\" \n",
        "MAX_INPUT_LENGTH = 512\n",
        "MAX_TARGET_LENGTH = 128\n",
        "TEST_BATCH_SIZE = 8\n",
        "ADAM_EPSILON = 1e-8 \n",
        "MAX_GRAD_NORM = 1.0 \n",
        "\n",
        "OUTPUT_DIR_BASE = \"./mt5_optuna_tuning_output\"\n",
        "COMBINED_TRAIN_DATA_PATH = \"combined_train_data.csv\"\n",
        "COMBINED_TEST_DATA_PATH = \"combined_test_data.csv\"\n",
        "DATA_DIR = \"data\" \n",
        "MIN_SUMMARY_WORDS = 3 \n",
        "LOG_INTERVAL = 100 \n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Optuna Config\n",
        "N_OPTUNA_TRIALS = 20 \n",
        "EPOCHS_PER_OPTUNA_TRIAL = 2 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Loading and Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text_series_for_metrics(text): \n",
        "    text = str(text).lower() \n",
        "    text = re.sub(r'\\s+', ' ', text) \n",
        "    text = re.sub(r'[^\\w\\s]', '', text) \n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def create_combined_datasets_if_not_exist():\n",
        "    # ... (Content from previous version of this function) ...\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        os.makedirs(DATA_DIR)\n",
        "        print(f\"Created directory: {DATA_DIR}. Please upload your CSV files here.\")\n",
        "    if os.path.exists(COMBINED_TRAIN_DATA_PATH) and os.path.exists(COMBINED_TEST_DATA_PATH):\n",
        "        print(f\"Found existing combined datasets: {COMBINED_TRAIN_DATA_PATH} and {COMBINED_TEST_DATA_PATH}\")\n",
        "        return\n",
        "    print(f\"Combined datasets not found. Attempting to create them from CSVs in '{DATA_DIR}/' directory...\")\n",
        "    train_files_langs = {\n",
        "        \"english_train.csv\": \"en\", \"hindi_train.csv\": \"hi\",\n",
        "        \"gujrati_train.csv\": \"gu\", \"bengali_train.csv\": \"bn\",\n",
        "    }\n",
        "    test_files_langs = {\n",
        "        \"english_test.csv\": \"en\", \"hindi_test.csv\": \"hi\",\n",
        "        \"gujrati_test.csv\": \"gu\", \"bengali_test.csv\": \"bn\",\n",
        "    }\n",
        "    def process_files(file_lang_map, output_path, dataset_type):\n",
        "        if os.path.exists(output_path):\n",
        "            print(f\"Found existing combined {dataset_type} dataset: {output_path}\")\n",
        "            return\n",
        "        all_data = []\n",
        "        for file_name, lang in file_lang_map.items():\n",
        "            full_file_path = os.path.join(DATA_DIR, file_name)\n",
        "            if not os.path.exists(full_file_path):\n",
        "                print(f\"Missing file: {full_file_path}\")\n",
        "                continue\n",
        "            try:\n",
        "                df = pd.read_csv(full_file_path, encoding='utf-8', on_bad_lines='skip')\n",
        "                if 'Article' not in df.columns or 'Summary' not in df.columns:\n",
        "                    print(f\" 'Article' or 'Summary' column missing in {file_name}. Skipping.\")\n",
        "                    continue\n",
        "                df['Article'] = df['Article'].astype(str).str.strip()\n",
        "                df['Summary'] = df['Summary'].astype(str).str.strip()\n",
        "                df.dropna(subset=['Article', 'Summary'], inplace=True) \n",
        "                df = df[df['Article'].str.len() > 0] \n",
        "                df = df[df['Summary'].str.len() > 0] \n",
        "                df = df[df['Summary'].apply(lambda x: len(x.split()) >= MIN_SUMMARY_WORDS)]\n",
        "                if df.empty:\n",
        "                    continue\n",
        "                df['lang'] = lang\n",
        "                all_data.append(df[['Article', 'Summary', 'lang']])\n",
        "                print(f\"Loaded and processed for {dataset_type}: {file_name}, kept {len(df)} rows.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_name}: {e}\")\n",
        "        if not all_data:\n",
        "            print(f\"No data loaded for {dataset_type} after filtering.\")\n",
        "            return\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        if combined_df.empty:\n",
        "            print(f\"Combined {dataset_type} dataset is empty. Cannot save.\")\n",
        "            return\n",
        "        combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "        combined_df.to_csv(output_path, index=False)\n",
        "        print(f\"Combined {dataset_type} dataset created and saved to: {output_path} with {len(combined_df)} rows.\")\n",
        "    process_files(train_files_langs, COMBINED_TRAIN_DATA_PATH, \"train\")\n",
        "    process_files(test_files_langs, COMBINED_TEST_DATA_PATH, \"test\")\n",
        "\n",
        "def load_data(data_path):\n",
        "    # ... (Content from previous version of this function) ...\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"Data file not found: {data_path}\")\n",
        "        if data_path == COMBINED_TRAIN_DATA_PATH or data_path == COMBINED_TEST_DATA_PATH:\n",
        "             create_combined_datasets_if_not_exist() \n",
        "             if not os.path.exists(data_path): \n",
        "                 return None\n",
        "        else:\n",
        "            return None\n",
        "    try:\n",
        "        df = pd.read_csv(data_path)\n",
        "        print(f\"Successfully loaded data from {data_path}, shape: {df.shape}\")\n",
        "        return df\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"Warning: Data file {data_path} is empty.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summarization Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prefix_by_lang(lang):\n",
        "    return f\"summarize in {lang}: \"\n",
        "\n",
        "class SummarizationDataset(Dataset):\n",
        "    # ... (Content from previous version of this class) ...\n",
        "    def __init__(self, dataframe, tokenizer, max_input_len, max_target_len):\n",
        "        self.data = dataframe.copy()\n",
        "        self.data['Article'] = self.data['Article'].astype(str).str.strip()\n",
        "        self.data['Summary'] = self.data['Summary'].astype(str).str.strip()\n",
        "        initial_len = len(self.data)\n",
        "        self.data = self.data[self.data['Article'].str.len() > 0]\n",
        "        self.data = self.data[self.data['Summary'].str.len() > 0]\n",
        "        self.data = self.data[self.data['Summary'].apply(lambda x: len(x.split()) >= MIN_SUMMARY_WORDS)]\n",
        "        self.data = self.data.reset_index(drop=True)\n",
        "        if len(self.data) < initial_len:\n",
        "            print(f\"SummarizationDataset: Initialized with {len(self.data)} rows after filtering {initial_len - len(self.data)} empty/short entries.\")\n",
        "        if len(self.data) == 0:\n",
        "            print(\"CRITICAL: SummarizationDataset is empty after filtering. No data available.\")\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_len = max_input_len\n",
        "        self.max_target_len = max_target_len\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, index):\n",
        "        if index >= len(self.data):\n",
        "            raise IndexError(\"Index out of bounds in SummarizationDataset\")\n",
        "        row = self.data.iloc[index]\n",
        "        article_lang = row['lang']\n",
        "        article_text_raw = str(row['Article'])\n",
        "        summary_text_raw = str(row['Summary'])\n",
        "        input_text = prefix_by_lang(article_lang) + article_text_raw\n",
        "        target_text = summary_text_raw\n",
        "        input_enc = self.tokenizer(input_text, max_length=self.max_input_len, padding='do_not_pad', truncation=True, return_tensors=\"pt\")\n",
        "        target_enc = self.tokenizer(target_text, max_length=self.max_target_len, padding='do_not_pad', truncation=True, return_tensors=\"pt\")\n",
        "        input_ids = input_enc[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = input_enc[\"attention_mask\"].squeeze(0)\n",
        "        labels = target_enc[\"input_ids\"].squeeze(0).clone()\n",
        "        if labels.ndim == 0: labels = labels.unsqueeze(0)\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Custom Data Collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomSummarizationCollator:\n",
        "    # ... (Content from previous version of this class) ...\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizerBase, model: Optional[PreTrainedModel] = None, label_pad_token_id: int = -100, pad_to_multiple_of: Optional[int] = None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.label_pad_token_id = label_pad_token_id\n",
        "        self.pad_to_multiple_of = pad_to_multiple_of\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        input_ids_list = [feature[\"input_ids\"].tolist() for feature in features]\n",
        "        attention_mask_list = [feature[\"attention_mask\"].tolist() for feature in features]\n",
        "        padded_inputs = self.tokenizer.pad(\n",
        "            {\"input_ids\": input_ids_list},\n",
        "            padding=\"longest\", max_length=MAX_INPUT_LENGTH, \n",
        "            pad_to_multiple_of=self.pad_to_multiple_of, return_tensors=\"pt\"\n",
        "        )\n",
        "        padded_attention_masks = self.tokenizer.pad(\n",
        "            {\"input_ids\": attention_mask_list},\n",
        "            padding=\"longest\", max_length=MAX_INPUT_LENGTH,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of, return_tensors=\"pt\"\n",
        "        )[\"input_ids\"]\n",
        "        batch = {\n",
        "            \"input_ids\": padded_inputs[\"input_ids\"],\n",
        "            \"attention_mask\": padded_attention_masks\n",
        "        }\n",
        "        if \"labels\" in features[0] and features[0][\"labels\"] is not None:\n",
        "            labels_list = [feature[\"labels\"] for feature in features]\n",
        "            max_label_len = max(len(l) for l in labels_list)\n",
        "            if self.pad_to_multiple_of is not None:\n",
        "                max_label_len = ((max_label_len + self.pad_to_multiple_of - 1) // self.pad_to_multiple_of * self.pad_to_multiple_of)\n",
        "            padded_labels_list = []\n",
        "            for label_tensor in labels_list:\n",
        "                padding_len = max_label_len - len(label_tensor)\n",
        "                padded_tensor = torch.cat([label_tensor, torch.full((padding_len,), self.tokenizer.pad_token_id, dtype=label_tensor.dtype)])\n",
        "                padded_labels_list.append(padded_tensor)\n",
        "            labels_tensor = torch.stack(padded_labels_list)\n",
        "            if self.model is not None and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\"):\n",
        "                batch[\"decoder_input_ids\"] = self.model.prepare_decoder_input_ids_from_labels(labels=labels_tensor.clone())\n",
        "            else: \n",
        "                shifted_labels = labels_tensor.new_zeros(labels_tensor.shape)\n",
        "                shifted_labels[..., 1:] = labels_tensor[..., :-1].clone()\n",
        "                shifted_labels[..., 0] = self.tokenizer.pad_token_id \n",
        "                batch[\"decoder_input_ids\"] = shifted_labels\n",
        "            labels_tensor[labels_tensor == self.tokenizer.pad_token_id] = self.label_pad_token_id\n",
        "            batch[\"labels\"] = labels_tensor\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Metrics Calculation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "def calculate_jaccard(str1, str2):\n",
        "    s1_cleaned = clean_text_series_for_metrics(str1)\n",
        "    s2_cleaned = clean_text_series_for_metrics(str2)\n",
        "    tokens1 = set(nltk.word_tokenize(s1_cleaned))\n",
        "    tokens2 = set(nltk.word_tokenize(s2_cleaned))\n",
        "    if not tokens1 and not tokens2: return 1.0\n",
        "    if not tokens1 or not tokens2: return 0.0\n",
        "    return len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))\n",
        "\n",
        "def calculate_cosine_tfidf(list_of_references, list_of_predictions):\n",
        "    if not list_of_references or not list_of_predictions or len(list_of_references) != len(list_of_predictions):\n",
        "        return 0.0\n",
        "    cleaned_refs = [clean_text_series_for_metrics(s) for s in list_of_references if str(s).strip()]\n",
        "    cleaned_preds = [clean_text_series_for_metrics(s) for s in list_of_predictions if str(s).strip()]\n",
        "    if not cleaned_refs or not cleaned_preds: return 0.0\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    corpus = cleaned_refs + cleaned_preds\n",
        "    try:\n",
        "        vectorizer.fit(corpus)\n",
        "    except ValueError:\n",
        "        print(\"Warning: TF-IDF Vectorizer could not be fitted (empty corpus after cleaning?).\")\n",
        "        return 0.0\n",
        "    total_similarity = 0\n",
        "    count = 0\n",
        "    for i in range(len(list_of_references)):\n",
        "        ref = clean_text_series_for_metrics(list_of_references[i])\n",
        "        pred = clean_text_series_for_metrics(list_of_predictions[i])\n",
        "        if not ref or not pred: continue\n",
        "        try:\n",
        "            tfidf_ref = vectorizer.transform([ref])\n",
        "            tfidf_pred = vectorizer.transform([pred])\n",
        "            sim = cosine_similarity(tfidf_ref, tfidf_pred)[0, 0]\n",
        "            total_similarity += sim\n",
        "            count += 1\n",
        "        except ValueError:\n",
        "            print(f\"Skipping cosine for pair due to empty vector: REF='{ref}', PRED='{pred}'\")\n",
        "            continue\n",
        "    return (total_similarity / count) * 100 if count > 0 else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training and Evaluation Loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_model_for_optuna(model, dataloader, tokenizer, device):\n",
        "    \"\"\"Simplified evaluation for Optuna, returns primary metric (e.g., ROUGE-1).\"\"\"\n",
        "    model.eval()\n",
        "    all_decoded_preds = []\n",
        "    all_decoded_labels = []\n",
        "    total_eval_loss = 0 \n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            \n",
        "            generated_ids = model.generate(\n",
        "                input_ids=input_ids, attention_mask=attention_mask,\n",
        "                max_length=MAX_TARGET_LENGTH, num_beams=4, early_stopping=True\n",
        "            )\n",
        "            decoded_preds_batch = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            labels_for_decode = labels.clone()\n",
        "            labels_for_decode[labels_for_decode == -100] = tokenizer.pad_token_id\n",
        "            decoded_labels_batch = tokenizer.batch_decode(labels_for_decode, skip_special_tokens=True)\n",
        "            all_decoded_preds.extend([pred.strip() for pred in decoded_preds_batch])\n",
        "            all_decoded_labels.extend([label.strip() for label in decoded_labels_batch])\n",
        "\n",
        "    rouge_preds = [pred if pred else \"<empty>\" for pred in all_decoded_preds]\n",
        "    rouge_labels = [label if label else \"<empty>\" for label in all_decoded_labels]\n",
        "    \n",
        "    if not rouge_preds or not rouge_labels:\n",
        "        return 0.0 \n",
        "\n",
        "    rouge_results = rouge_metric.compute(predictions=rouge_preds, references=rouge_labels, use_stemmer=True)\n",
        "    return rouge_results.get('rouge1', 0.0) * 100 \n",
        "\n",
        "def objective(trial: optuna.trial.Trial):\n",
        "    gc.collect()\n",
        "    if DEVICE == torch.device(\"cuda\"):\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\nStarting Optuna Trial: {trial.number}\")\n",
        "    lr = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n",
        "    trial_epochs = trial.suggest_int(\"num_train_epochs\", 1, EPOCHS_PER_OPTUNA_TRIAL) # Short epochs for trials\n",
        "    trial_weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-1, log=True)\n",
        "    trial_train_batch_size = trial.suggest_categorical(\"train_batch_size\", [2, 4, 8])\n",
        "    trial_warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.0, 0.2)\n",
        "\n",
        "    print(f\"  Trial {trial.number} Params: LR={lr:.2e}, Epochs={trial_epochs}, WD={trial_weight_decay:.2e}, BS={trial_train_batch_size}\")\n",
        "\n",
        "    current_tokenizer = MT5Tokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
        "    current_model = MT5ForConditionalGeneration.from_pretrained(MODEL_NAME_OR_PATH)\n",
        "    current_model.to(DEVICE)\n",
        "\n",
        "    \n",
        "    current_train_df, current_val_df = train_test_split(df_train_val_global, test_size=0.1, random_state=42)\n",
        "    current_train_dataset = SummarizationDataset(current_train_df, current_tokenizer, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH)\n",
        "    current_val_dataset = SummarizationDataset(current_val_df, current_tokenizer, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH)\n",
        "\n",
        "    if len(current_train_dataset) == 0 or len(current_val_dataset) == 0:\n",
        "        print(f\"Warning: Trial {trial.number} has empty train or val dataset after init. Skipping.\")\n",
        "        return 0.0 \n",
        "\n",
        "    current_collator = CustomSummarizationCollator(tokenizer=current_tokenizer, model=current_model, label_pad_token_id=-100)\n",
        "    current_train_dataloader = DataLoader(current_train_dataset, batch_size=trial_train_batch_size, collate_fn=current_collator, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    current_val_dataloader = DataLoader(current_val_dataset, batch_size=trial_train_batch_size, collate_fn=current_collator, num_workers=2, pin_memory=True)\n",
        "\n",
        "    if len(current_train_dataloader) == 0 or len(current_val_dataloader) == 0:\n",
        "        print(f\"Warning: Trial {trial.number} has empty train or val dataloader. Skipping.\")\n",
        "        return 0.0\n",
        "\n",
        "    optimizer = AdamW(current_model.parameters(), lr=lr, eps=ADAM_EPSILON, weight_decay=trial_weight_decay)\n",
        "    total_steps = len(current_train_dataloader) * trial_epochs\n",
        "    # num_warmup = int(trial_warmup_ratio * total_steps)\n",
        "    num_warmup = int(0.1 * total_steps)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup, num_training_steps=total_steps)\n",
        "\n",
        "    for epoch in range(trial_epochs):\n",
        "        current_model.train()\n",
        "        print(f\"  Trial {trial.number}, Epoch {epoch+1}/{trial_epochs} Training...\")\n",
        "        for batch_idx, batch in enumerate(current_train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            labels = batch['labels'].to(DEVICE)\n",
        "            decoder_input_ids = batch.get('decoder_input_ids', None)\n",
        "            if decoder_input_ids is not None: decoder_input_ids = decoder_input_ids.to(DEVICE)\n",
        "\n",
        "            if (labels != -100).sum() == 0: \n",
        "                continue\n",
        "\n",
        "            outputs = current_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, decoder_input_ids=decoder_input_ids)\n",
        "            loss = outputs.loss\n",
        "            if loss is None or torch.isnan(loss) or torch.isinf(loss):\n",
        "                print(f\"    Trial {trial.number} WARNING: Invalid loss at step {batch_idx}. Skipping.\")\n",
        "                return 0.0\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(current_model.parameters(), MAX_GRAD_NORM)\n",
        "            optimizer.step()\n",
        "            if scheduler: scheduler.step()\n",
        "\n",
        "            if batch_idx % LOG_INTERVAL == 0:\n",
        "                print(f\"    Trial {trial.number}, Ep {epoch+1}, Batch {batch_idx+1}/{len(current_train_dataloader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"  Trial {trial.number} Evaluating...\")\n",
        "    validation_rouge1 = evaluate_model_for_optuna(current_model, current_val_dataloader, current_tokenizer, DEVICE)\n",
        "    print(f\"  Trial {trial.number} Validation ROUGE-1: {validation_rouge1:.4f}\")\n",
        "    \n",
        "    del current_model, current_tokenizer, current_train_dataset, current_val_dataset, current_train_dataloader, current_val_dataloader, optimizer, scheduler\n",
        "    gc.collect()\n",
        "    if DEVICE == torch.device(\"cuda\"):\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return validation_rouge1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Prepare Global Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_combined_datasets_if_not_exist()\n",
        "df_train_val_global = load_data(COMBINED_TRAIN_DATA_PATH)\n",
        "df_test_global = load_data(COMBINED_TEST_DATA_PATH)\n",
        "\n",
        "if df_train_val_global is None or df_train_val_global.empty:\n",
        "    print(\"STOPPING: Global training data for Optuna could not be loaded or is empty.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Run Optuna Hyperparameter Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'df_train_val_global' in globals() and df_train_val_global is not None and not df_train_val_global.empty:\n",
        "    study = optuna.create_study(direction=\"maximize\", study_name=\"mt5_summarization_tuning\")\n",
        "  \n",
        "    try:\n",
        "        study.optimize(objective, n_trials=N_OPTUNA_TRIALS, timeout=3600*4) # Example timeout of 4 hours\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Optuna optimization: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\nOptuna Hyperparameter Search Finished.\")\n",
        "    print(\"Best trial:\")\n",
        "    best_trial = study.best_trial\n",
        "    print(f\"  Value (Validation ROUGE-1): {best_trial.value:.4f}\")\n",
        "    print(\"  Params: \")\n",
        "    for key, value in best_trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n",
        "    \n",
        "    # Save best params\n",
        "    if not os.path.exists(OUTPUT_DIR_BASE):\n",
        "        os.makedirs(OUTPUT_DIR_BASE)\n",
        "    best_params_file = os.path.join(OUTPUT_DIR_BASE, \"best_hyperparameters.json\")\n",
        "    with open(best_params_file, \"w\") as f:\n",
        "        json.dump(best_trial.params, f, indent=4)\n",
        "    print(f\"Best hyperparameters saved to {best_params_file}\")\n",
        "else:\n",
        "    print(\"Global training data not loaded. Skipping Optuna hyperparameter search.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Train & Evaluate Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_final_training_and_evaluation(best_params, full_num_epochs):\n",
        "    print(\"\\n--- Starting Final Training with Best Hyperparameters ---\")\n",
        "    print(f\"Best Params: {best_params}\")\n",
        "    print(f\"Training for {full_num_epochs} epochs.\")\n",
        "\n",
        "    final_output_dir = os.path.join(OUTPUT_DIR_BASE, \"final_model_with_best_params\")\n",
        "    if not os.path.exists(final_output_dir):\n",
        "        os.makedirs(final_output_dir)\n",
        "\n",
        "    final_tokenizer = MT5Tokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
        "    final_model = MT5ForConditionalGeneration.from_pretrained(MODEL_NAME_OR_PATH)\n",
        "    final_model.to(DEVICE)\n",
        "\n",
        "    if 'df_train_val_global' not in globals() or df_train_val_global is None or df_train_val_global.empty:\n",
        "        print(\"Global training data not available for final training. Exiting.\")\n",
        "        return\n",
        "    \n",
        "    current_train_df, current_val_df = train_test_split(df_train_val_global, test_size=0.1, random_state=42)\n",
        "    final_train_dataset = SummarizationDataset(current_train_df, final_tokenizer, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH)\n",
        "    final_val_dataset = SummarizationDataset(current_val_df, final_tokenizer, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH)\n",
        "\n",
        "    if len(final_train_dataset) == 0 or len(final_val_dataset) == 0:\n",
        "        print(\"Final train or val dataset is empty. Exiting final training.\")\n",
        "        return\n",
        "\n",
        "    final_collator = CustomSummarizationCollator(tokenizer=final_tokenizer, model=final_model, label_pad_token_id=-100)\n",
        "    final_train_dataloader = DataLoader(final_train_dataset, batch_size=best_params.get('train_batch_size', TRAIN_BATCH_SIZE), collate_fn=final_collator, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    final_val_dataloader = DataLoader(final_val_dataset, batch_size=best_params.get('train_batch_size', VALID_BATCH_SIZE), collate_fn=final_collator, num_workers=2, pin_memory=True)\n",
        "\n",
        "    if len(final_train_dataloader) == 0 or len(final_val_dataloader) == 0:\n",
        "        print(\"Final train or val dataloader is empty. Exiting final training.\")\n",
        "        return\n",
        "\n",
        "    final_optimizer = AdamW(final_model.parameters(), \n",
        "                            lr=best_params['learning_rate'], \n",
        "                            eps=ADAM_EPSILON, \n",
        "                            weight_decay=best_params['weight_decay'])\n",
        "    final_total_steps = len(final_train_dataloader) * full_num_epochs\n",
        "    final_warmup_ratio = 0.1 \n",
        "    final_num_warmup = int(final_warmup_ratio * final_total_steps)\n",
        "    final_scheduler = get_linear_schedule_with_warmup(final_optimizer, num_warmup_steps=final_num_warmup, num_training_steps=final_total_steps)\n",
        "\n",
        "    global OUTPUT_DIR\n",
        "    original_output_dir = OUTPUT_DIR\n",
        "    OUTPUT_DIR = final_output_dir\n",
        "\n",
        "    print(f\"Training final model with best params, outputting to: {final_output_dir}\")\n",
        "    train_model(final_model, final_train_dataloader, final_val_dataloader, final_optimizer, final_scheduler, full_num_epochs, DEVICE)\n",
        "\n",
        "    OUTPUT_DIR = original_output_dir\n",
        "\n",
        "    # Evaluate on Test Set\n",
        "    if 'df_test_global' in globals() and df_test_global is not None and not df_test_global.empty:\n",
        "        print(\"\\n--- Evaluating Final Model on Test Set ---\")\n",
        "        model_for_test_eval = MT5ForConditionalGeneration.from_pretrained(final_output_dir)\n",
        "        model_for_test_eval.to(DEVICE)\n",
        "        tokenizer_for_test_eval = MT5Tokenizer.from_pretrained(final_output_dir)\n",
        "\n",
        "        final_test_dataset = SummarizationDataset(df_test_global, tokenizer_for_test_eval, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH)\n",
        "        if len(final_test_dataset) > 0:\n",
        "            final_test_collator = CustomSummarizationCollator(tokenizer=tokenizer_for_test_eval, model=model_for_test_eval, label_pad_token_id=-100)\n",
        "            final_test_dataloader = DataLoader(final_test_dataset, batch_size=TEST_BATCH_SIZE, collate_fn=final_test_collator)\n",
        "            if len(final_test_dataloader) > 0:\n",
        "                original_output_dir_eval = OUTPUT_DIR\n",
        "                OUTPUT_DIR = final_output_dir \n",
        "                evaluate_model(model_for_test_eval, final_test_dataloader, tokenizer_for_test_eval, DEVICE, is_test_set=True)\n",
        "                OUTPUT_DIR = original_output_dir_eval\n",
        "            else:\n",
        "                print(\"Final test dataloader is empty.\")\n",
        "        else:\n",
        "            print(\"Final test dataset is empty.\")\n",
        "    else:\n",
        "        print(\"Global test data not available or empty. Skipping final test set evaluation.\")\n",
        "\n",
        "# --- Main Execution for Optuna --- \n",
        "if 'df_train_val_global' in globals() and df_train_val_global is not None and not df_train_val_global.empty:\n",
        "    study_db_path = f\"sqlite:///{os.path.join(OUTPUT_DIR_BASE, 'optuna_study.db')}\"\n",
        "    best_params_from_file = None\n",
        "    best_params_file_path = os.path.join(OUTPUT_DIR_BASE, \"best_hyperparameters.json\")\n",
        "\n",
        "    if os.path.exists(best_params_file_path):\n",
        "        print(f\"Found existing best hyperparameters file: {best_params_file_path}\")\n",
        "        with open(best_params_file_path, 'r') as f:\n",
        "            best_params_from_file = json.load(f)\n",
        "    \n",
        "    if best_params_from_file:\n",
        "        print(\"Using previously found best hyperparameters for final training.\")\n",
        "        run_final_training_and_evaluation(best_params_from_file, full_num_epochs=NUM_TRAIN_EPOCHS) # Use original NUM_TRAIN_EPOCHS for full run\n",
        "    else:\n",
        "        print(f\"No existing best hyperparameters found. Running Optuna study: {study_db_path}\")\n",
        "        if not os.path.exists(OUTPUT_DIR_BASE):\n",
        "            os.makedirs(OUTPUT_DIR_BASE)\n",
        "        study = optuna.create_study(direction=\"maximize\", \n",
        "                                    study_name=\"mt5_summarization_tuning\", \n",
        "                                    storage=study_db_path, \n",
        "                                    load_if_exists=True)\n",
        "        try:\n",
        "            study.optimize(objective, n_trials=N_OPTUNA_TRIALS, timeout=3600*6) \n",
        "        except Exception as e:\n",
        "            print(f\"Error during Optuna optimization: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "        print(\"\\nOptuna Hyperparameter Search Finished.\")\n",
        "        if len(study.trials) > 0:\n",
        "            print(\"Best trial:\")\n",
        "            best_trial = study.best_trial\n",
        "            print(f\"  Value (Validation ROUGE-1): {best_trial.value:.4f}\")\n",
        "            print(\"  Params: \")\n",
        "            for key, value in best_trial.params.items():\n",
        "                print(f\"    {key}: {value}\")\n",
        "            with open(best_params_file_path, \"w\") as f:\n",
        "                json.dump(best_trial.params, f, indent=4)\n",
        "            print(f\"Best hyperparameters saved to {best_params_file_path}\")\n",
        "            # Run final training with these best params\n",
        "            run_final_training_and_evaluation(best_trial.params, full_num_epochs=NUM_TRAIN_EPOCHS)\n",
        "        else:\n",
        "            print(\"No trials completed in Optuna study.\")\n",
        "else:\n",
        "    print(\"Global training data not loaded. Skipping Optuna hyperparameter search and final training.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Example Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_model_dir = os.path.join(OUTPUT_DIR_BASE, \"final_model_with_best_params\")\n",
        "if os.path.exists(os.path.join(final_model_dir, \"pytorch_model.bin\")):\n",
        "    print(\"\\n--- Example Prediction using the BEST fine-tuned model ---\")\n",
        "    loaded_model = MT5ForConditionalGeneration.from_pretrained(final_model_dir)\n",
        "    loaded_model.to(DEVICE)\n",
        "    loaded_model.eval()\n",
        "    loaded_tokenizer = MT5Tokenizer.from_pretrained(final_model_dir) \n",
        "\n",
        "    sample_article_en = \"summarize in en: Several research groups have been working on developing new types of batteries that could store more energy and charge faster. One promising approach involves using solid-state electrolytes instead of liquid ones, which could improve safety and energy density. These advancements are crucial for the future of electric vehicles and portable electronics.\"\n",
        "    sample_article_hi = \"summarize in hi: कई शोध समूह नई प्रकार की बैटरियों को विकसित करने पर काम कर रहे हैं जो अधिक ऊर्जा संग्रहीत कर सकें और तेजी से चार्ज हो सकें। एक आशाजनक दृष्टिकोण में तरल इलेक्ट्रोलाइट्स के बजाय ठोस-अवस्था वाले इलेक्ट्रोलाइट्स का उपयोग करना शामिल है, जिससे सुरक्षा और ऊर्जा घनत्व में सुधार हो सकता है। ये प्रगति इलेक्ट्रिक वाहनों और पोर्टेबल इलेक्ट्रॉनिक्स के भविष्य के लिए महत्वपूर्ण हैं।\"\n",
        "\n",
        "    for lang_code, sample_article in [(\"en\", sample_article_en), (\"hi\", sample_article_hi)]:\n",
        "        print(f\"\\nInput Article ({lang_code}): {sample_article.replace(f'summarize in {lang_code}: ', '')}\")\n",
        "        inputs = loaded_tokenizer(sample_article, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True, padding=True).to(DEVICE)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            summary_ids = loaded_model.generate(\n",
        "                inputs.input_ids,\n",
        "                attention_mask=inputs.attention_mask,\n",
        "                num_beams=4,\n",
        "                max_length=MAX_TARGET_LENGTH,\n",
        "                early_stopping=True\n",
        "            )\n",
        "        generated_summary = loaded_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        print(f\"Generated Summary ({lang_code}): {generated_summary}\")\n",
        "else:\n",
        "    print(f\"Fine-tuned model not found in {final_model_dir}. Skipping example prediction.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
